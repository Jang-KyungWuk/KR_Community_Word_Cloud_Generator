{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Analyzer v 1.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8OMHuNn1447B2oHBOkoI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jang-KyungWuk/KR_Community_Word_Cloud_Generator/blob/master/Keyword%20Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F19674Lupe4w",
        "outputId": "e2cd45c4-134c-4642-f734-701e6498acdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "<current version 1.2>\n",
        "Issue that DC article crawling is not working properly has been fixed\n",
        "Keyword Analyzer now use konlpy version 0.5.1 (legacy version)\n",
        "Emoji and more than 1 blank is now deleted BEFORE morpheme analyze\n",
        "\n",
        "<future version 1.3 will try>\n",
        "Test Ruliweb crawling and fix if there're bugs\n",
        "Add word2vec feature (can be postponded)\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n<current version 1.2>\\nIssue that DC article crawling is not working properly has been fixed\\n\\n<future version 1.3 will try>\\nTest Ruliweb crawling and fix if there're bugs\\nAdd word2vec feature (can be postponded)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBUVudqsOcl8"
      },
      "source": [
        "def setup():\n",
        "  '''\n",
        "  FOR GOOGLE COLAB USERS START setup() \n",
        "  AND RESTART RUNTIME (CTRL+M) \n",
        "  THEN START setup() TO CHECK WHETHER KR FONT IS PRINTED CORRECTLY\n",
        "  '''\n",
        "  \n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  base_dir=\"/content/gdrive/My Drive/\"\n",
        "\n",
        "  # KoNLPy Install\n",
        "  !apt-get update\n",
        "  !apt-get install g++ openjdk-8-jdk \n",
        "  !pip3 install konlpy==0.5.1\n",
        "\n",
        "  # Install KR Font\n",
        "  !apt -qq -y install fonts-nanum\n",
        "  fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "  KR_Font_initializer(fontpath)\n",
        "\n",
        "  # Install emoji to delete emojis\n",
        "  !pip install emoji\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def KR_Font_initializer(fontpath):\n",
        "  # For draw legend\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib as mpl\n",
        "  import matplotlib.font_manager as fm\n",
        "\n",
        "  # Set KR Font for matplotlib\n",
        "  font=fm.FontProperties(fname=fontpath,size=20)\n",
        "  plt.rc('font',family='NanumBarunGothic')\n",
        "  mpl.font_manager._rebuild()\n",
        "\n",
        "  plt.bar(['KR font','성공'],[1,2])\n",
        "  plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZYGIatVeHZ"
      },
      "source": [
        "def get_current_time_code():\n",
        "  '''time_code()\n",
        "  Generate time code from current time\n",
        "  '''\n",
        "\n",
        "  # Get current time (KST = GMT +9)\n",
        "  KST=datetime.timezone(datetime.timedelta(hours=9))\n",
        "  now=datetime.datetime.now(KST)\n",
        "  MM=str(now.month)\n",
        "  DD=str(now.day)\n",
        "  hh=str(now.hour)\n",
        "  mm=str(now.minute)\n",
        "  ss=str(now.second)\n",
        "  c_time=[MM,DD,hh,mm,ss]\n",
        "  for cursor in range (len(c_time)):\n",
        "    if len(c_time[cursor])==1:\n",
        "      c_time[cursor]=\"0\"+c_time[cursor]\n",
        "  time_code='-'.join(c_time)\n",
        "\n",
        "  return time_code"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mEKZYziS_j"
      },
      "source": [
        "# Function for coloring\n",
        "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "  return tuple(Dark2_8.colors[random.randint(0,7)])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izP03y58f4Tf"
      },
      "source": [
        "# Function to find titles at DCinside\n",
        "def dc_title_finder(tag):\n",
        "  return tag.has_attr('href') and not tag.has_attr('class') and not tag.has_attr('rel')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1YtK8QKT3q"
      },
      "source": [
        "def dcinside_crawl(pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir):\n",
        "\n",
        "  main_url=\"https://gall.dcinside.com\"\n",
        "  base_url=main_url*1\n",
        "  if is_m_gallery==True:\n",
        "    base_url+=\"/mgallery\"\n",
        "  base_url+=((\"/board/lists/?id=%s&page=\")%board_id)\n",
        "\n",
        "  title_tag_pos_min, title_tag_pos_max=dc_title_tag_pos[0]+dc_official_announce, dc_title_tag_pos[1]+dc_official_announce\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "      html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      tags=soup.find_all(dc_title_finder)\n",
        "      bbs_tags=tags[title_tag_pos_min:title_tag_pos_max]\n",
        "      for t in bbs_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(main_url+t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "        html=rq.get(dir, headers={'User-Agent':'el anemos keyword analyzer for word cloud'}).text\n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article=soup.body.find('div',{'style':'overflow:hidden;width:900px'}).text\n",
        "        article=article.replace('\\n',' ').replace('\\u200b',' ').replace('\\xa0','').replace('- dc official App','')\n",
        "        article_crawl_txt.write(article+'\\n')\n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRL7tsHKR_0h"
      },
      "source": [
        "def ruliweb_crawl(pages,txt_files,crawl_title,crawl_article,board_id,request_delay,user_agent,base_dir):\n",
        "\n",
        "  # For web crawling\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests as rq\n",
        "  import re\n",
        "  \n",
        "  main_url=\"https://bbs.ruliweb.com/community/board\"\n",
        "  base_url=main_url*1\n",
        "  base_url+=((\"/%s?page=\")%board_id)\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "      html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      title_soup=soup.find_all('a', attrs={'class':'deco'})\n",
        "      title_tags=title_soup[len(title_soup)-32 : len(title_soup)-4]\n",
        "\n",
        "      for t in bbs_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(main_url+t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "        html=rq.get(dir, headers={'User-Agent':'el anemos keyword analyzer for word cloud'}).text\n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article_tags=soup.find_all(\"div\",class_=\"view_content\")\n",
        "        article_text=''\n",
        "        for article_tag in article_tags:\n",
        "          article_text+=(article_tag.text+' ')\n",
        "        article_text=article_text.replace('\\u200b',' ')\n",
        "        article_crawl_txt.write(article_text+'\\n')\n",
        "        \n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsLvoTh6VI3f"
      },
      "source": [
        "def name_txt_files(site,board_id,pages,time_code, crawl_title, crawl_article):\n",
        "  txt_files=[[],[],[]] #Title, link, article\n",
        "\n",
        "  if crawl_title==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[0].append((\"%s %s title crawl %s %s~%s.txt\")%(site, board_id, time_code, crawl_min, crawl_max-1))\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[1].append((\"%s %s link crawl %s %s~%s.txt\")%(site, board_id, time_code, crawl_min, crawl_max-1))\n",
        "      txt_files[2].append((\"%s %s article crawl %s %s~%s.txt\")%(site, board_id, time_code, crawl_min, crawl_max-1))\n",
        "  \n",
        "  return txt_files"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0xx7hdPsTa8"
      },
      "source": [
        "def wordseed_reader(file_names,crawl_title,crawl_article,base_dir,pages):\n",
        "  wordseeds=[]\n",
        "  len_pages=len(pages)\n",
        "\n",
        "  for crawl_sector_cursor in range (len_pages):\n",
        "    wordseed=\"\"\n",
        "    if crawl_title==True:\n",
        "        title_crawl_txt=open(base_dir+file_names[0][crawl_sector_cursor])\n",
        "        wordseed+=title_crawl_txt.read()\n",
        "        title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "        article_crawl_txt=open(base_dir+file_names[2][crawl_sector_cursor])\n",
        "        wordseed+=article_crawl_txt.read()\n",
        "        article_crawl_txt.close()\n",
        "    wordseed=blank_emoji_deleter(wordseed)\n",
        "    wordseeds.append(wordseed)\n",
        "\n",
        "  return wordseeds"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYNh3F5D3iB"
      },
      "source": [
        "def blank_emoji_deleter(wordseed):\n",
        "  # Delete '\\n (Enter key input)'\n",
        "  wordseed=wordseed.replace('\\s+',' ').replace('\\n',' ').replace('\\u3000',' ')\n",
        "\n",
        "  # Delete Emoji\n",
        "  wordseed=emoji.get_emoji_regexp().sub(u'',wordseed)\n",
        "\n",
        "  return wordseed"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPfI_glewXh_"
      },
      "source": [
        "def morpheme_analysis(wordseed,user_dic,base_dir):\n",
        "  tagged_words=konlpy.tag.Komoran(userdic=base_dir+user_dic).pos(wordseed)\n",
        "  grammar=\"\"\"\n",
        "NP: {<N.*>*<Suffix>?}   # Noun phrase\n",
        "VP: {<V.*>*}            # Verb phrase\n",
        "AP: {<A.*>*}            # Adjective phrase\n",
        "\"\"\"\n",
        "  parser=nltk.RegexpParser(grammar)\n",
        "\n",
        "  return tagged_words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp2SzMygE7yq"
      },
      "source": [
        "def morpheme_tag_filtter(tagged_words,selected_tags):\n",
        "  filtered_word_seed=\"\"\n",
        "  for morpheme in tagged_words:\n",
        "    if morpheme[1] in selected_tags:\n",
        "      filtered_word_seed+=(\"%s \"%morpheme[0]) \n",
        "  return filtered_word_seed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQn2gb8rvoNM"
      },
      "source": [
        "def keyword_ranking(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  wordseed_list=wordseed.split()\n",
        "  keywords=[]\n",
        "  counts=[]\n",
        "\n",
        "  for keyword in wordseed_list:\n",
        "    if keyword in keywords:\n",
        "      counts[keywords.index(keyword)]+=1\n",
        "    else:\n",
        "      keywords.append(keyword)\n",
        "      counts.append(1)\n",
        "\n",
        "  plt_kw=[]\n",
        "  plt_ct=[]\n",
        "\n",
        "  for rank_cursor in range (show_rank):\n",
        "    index=counts.index(max(counts))\n",
        "    plt_kw.append(keywords.pop(index))\n",
        "    plt_ct.append(counts.pop(index))\n",
        "  plt_kw.reverse()\n",
        "  plt_ct.reverse()\n",
        "  plt.barh(plt_kw,plt_ct)\n",
        "  plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\n",
        "  plt.xlabel(\"등장 횟수\")\n",
        "  plt.savefig(base_dir+(\"%s %s Keyword Ranking %s %s~%s.png\"%(site,board_id,time_code,crawl_min,crawl_max-1)))\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXQ7oogUM3U"
      },
      "source": [
        "def wordcloud_generator(wordseed,mask_image, site, board_id, time_code, crawl_page,fontpath,base_dir):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  # Word cloud generate (Komoran+user_dic)\n",
        "  icon=Image.open(base_dir+mask_image)\n",
        "  mask=Image.new(\"RGB\",icon.size, (255,255,255))\n",
        "  mask.paste(icon,icon)\n",
        "  mask=np.array(mask)\n",
        "\n",
        "  wc=WordCloud(font_path=fontpath, background_color=\"white\",max_words=150,mask=mask,max_font_size=400,random_state=1234567)\n",
        "\n",
        "  wc.generate_from_text(wordseed)\n",
        "  wc.recolor(color_func=color_func, random_state=1234567)\n",
        "    \n",
        "  wc_dir=base_dir+(\"%s %s Word Cloud %s %s~%s.png\")%(site, board_id,time_code,crawl_min,crawl_max-1)\n",
        "  wc.to_file(wc_dir)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0qmrOjBdTMB"
      },
      "source": [
        "def Keyword_analyzer(site,board_id,crawl_pages,cloud_mask,crawl_title=True, crawl_article=False,\n",
        "                     show_ranking=True,show_rank=5, is_m_gallery=False,\n",
        "                     custom_word_corpus=\"\",base_dir=\"\", request_delay=1,\n",
        "                     dc_title_tag_pos=[21,71],dc_official_announce=0,\n",
        "                     user_agent=\"'el anemos keyword analyzer for word cloud | contact elixir95@naver.com\",\n",
        "                     random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                     selected_tags=[\"NNG\",\"NNP\",\"NR\",\"IC\",\"SL\",\"SH\",\"SW\",\"SN\"]):\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\n",
        "  All in one function that do webcrawl, create wordcloud, legend\n",
        "  site supports dcinside, ruliweb\n",
        "  '''\n",
        "\n",
        "  time_code=get_current_time_code()\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir)\n",
        "\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,request_delay,user_agent,base_dir)\n",
        "\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages)\n",
        "\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "    if show_ranking==True:\n",
        "      keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\n",
        "    \n",
        "    wordcloud_generator(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrbTb9y9M20n",
        "cellView": "both"
      },
      "source": [
        "# Download libraries\n",
        "base_dir=setup()\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "# For web crawling\n",
        "from bs4 import BeautifulSoup\n",
        "import requests as rq\n",
        "import re\n",
        "\n",
        "# For naming\n",
        "import datetime\n",
        "\n",
        "# For word cloud generating\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from palettable.colorbrewer.qualitative import Dark2_8\n",
        "\n",
        "# For kr analysis\n",
        "import konlpy\n",
        "import nltk\n",
        "\n",
        "# For set delay\n",
        "import time\n",
        "\n",
        "# For emoji delete\n",
        "import emoji\n",
        "\n",
        "# For draw legend\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV_v7lgrcN6s"
      },
      "source": [
        "Keyword_analyzer(site='dc',\n",
        "                 board_id='hypergryph',\n",
        "                 crawl_pages=[[2,3]],\n",
        "                 crawl_title=False, \n",
        "                 crawl_article=True,\n",
        "                 show_ranking=True,\n",
        "                 show_rank=10,\n",
        "                 is_m_gallery=True,\n",
        "                 cloud_mask='blemishine_b.png',\n",
        "                 custom_word_corpus=\"dc_ark_user_dic_v20201018.txt\",\n",
        "                 dc_official_announce=1,\n",
        "                 base_dir=base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}