{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Analyzer v 2.2f.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMytd2gXpZYUvsS7SNbub1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jang-KyungWuk/KR_Community_Word_Cloud_Generator/blob/master/Keyword%20Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F19674Lupe4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ae336f1d-2560-47a3-c6cb-75750ad0d6c1"
      },
      "source": [
        "version=2.2\n",
        "'''\n",
        "<current version 2.2>\n",
        "Added new algorithm \n",
        "  - recover_known_Keyword_wordcloud() : draw wordloud with only given keywords\n",
        "Fixed article crawl on dcinside.com\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n<current version 2.1>\\n~1.9 version algorithm available again.\\n(add _v1dx to function name)\\n\\n<future version 2.2 will try>\\nHyperlink based redundant-safe title crawl\\nAdd word2vec or LDA feature (can be postponded. needs futher study)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBUVudqsOcl8"
      },
      "source": [
        "def setup():\n",
        "  '''\n",
        "  FOR GOOGLE COLAB USERS START setup() \n",
        "  AND RESTART RUNTIME (CTRL+M) \n",
        "  THEN START setup() TO CHECK WHETHER KR FONT IS PRINTED CORRECTLY\n",
        "  '''\n",
        "  \n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  base_dir=\"/content/gdrive/My Drive/\"\n",
        "\n",
        "  # KoNLPy Install\n",
        "  !apt-get update\n",
        "  !apt-get install g++ openjdk-8-jdk \n",
        "  !pip3 install konlpy==0.5.1\n",
        "\n",
        "  # Install KR Font\n",
        "  !apt -qq -y install fonts-nanum\n",
        "  fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "  KR_Font_initializer(fontpath)\n",
        "\n",
        "  # Install emoji to delete emojis\n",
        "  !pip install emoji\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def KR_Font_initializer(fontpath):\n",
        "  # For draw legend\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib as mpl\n",
        "  import matplotlib.font_manager as fm\n",
        "\n",
        "  # Set KR Font for matplotlib\n",
        "  font=fm.FontProperties(fname=fontpath,size=20)\n",
        "  plt.rc('font',family='NanumBarunGothic')\n",
        "  mpl.font_manager._rebuild()\n",
        "\n",
        "  plt.bar(['KR font','성공'],[1,2])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZYGIatVeHZ"
      },
      "source": [
        "def get_current_time_code():\n",
        "  '''time_code()\n",
        "  Generate time code from current time\n",
        "  '''\n",
        "\n",
        "  # Get current time (KST = GMT +9)\n",
        "  KST=datetime.timezone(datetime.timedelta(hours=9))\n",
        "  now=datetime.datetime.now(KST)\n",
        "  YY=str(now.year)[2:4]\n",
        "  MM=str(now.month)\n",
        "  DD=str(now.day)\n",
        "\n",
        "  hh=str(now.hour)\n",
        "  mm=str(now.minute)\n",
        "  ss=str(now.second)\n",
        "  \n",
        "  c_time=[YY,MM,DD,hh,mm,ss]\n",
        "  for cursor in range (len(c_time)):\n",
        "    if len(c_time[cursor])==1:\n",
        "      c_time[cursor]=\"0\"+c_time[cursor]\n",
        "  time_code=''.join(c_time[0:3])+' '+'-'.join(c_time[3:6])\n",
        "\n",
        "  return time_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mEKZYziS_j"
      },
      "source": [
        "# Function for coloring\n",
        "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "  return tuple(Dark2_8.colors[random.randint(0,7)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izP03y58f4Tf"
      },
      "source": [
        "# Function to find titles at DCinside\n",
        "def dc_title_finder(tag):\n",
        "  return tag.has_attr('href') and not tag.has_attr('class') and not tag.has_attr('rel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4jxmdCHu5-1"
      },
      "source": [
        "def keywords_filter(tags,keywords):\n",
        "  filtered_tags=[]\n",
        "  for tag in tags:\n",
        "    if any(keyword in tag.text for keyword in keywords)==True:\n",
        "      filtered_tags.append(tag)\n",
        "  return filtered_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1YtK8QKT3q"
      },
      "source": [
        "def dcinside_crawl(pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir,\n",
        "                   filter=False,\n",
        "                   keywords=[]):\n",
        "\n",
        "  main_url=\"https://gall.dcinside.com\"\n",
        "  base_url=main_url*1\n",
        "  if is_m_gallery==True:\n",
        "    base_url+=\"/mgallery\"\n",
        "  base_url+=((\"/board/lists/?id=%s&page=\")%board_id)\n",
        "\n",
        "  title_tag_pos_min, title_tag_pos_max=dc_title_tag_pos[0]+dc_official_announce, dc_title_tag_pos[1]+dc_official_announce\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "\n",
        "      try:\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "        print(\"Requesting page %s once more\"%(page))\n",
        "        time.sleep(retry_delay)\n",
        "        try:\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Canceled requesting page %s\"%(page))\n",
        "      \n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      tags=soup.find_all(dc_title_finder)\n",
        "      bbs_tags=tags[title_tag_pos_min:title_tag_pos_max]\n",
        "      \n",
        "      if filter==True:\n",
        "        bbs_tags=keywords_filter(bbs_tags,keywords)\n",
        "\n",
        "      for t in bbs_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(main_url+t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "        \n",
        "        try:\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Requesting page %s once more\"%(dir))\n",
        "          time.sleep(retry_delay)\n",
        "          try:\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "            print(\"Canceled requesting page %s\"%(dir))\n",
        "        \n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article=soup.body.find('div',{'class':'writing_view_box'}).text\n",
        "        article=article.replace('\\n',' ').replace('\\u200b',' ').replace('\\xa0','').replace('- dc official App','')\n",
        "        article_crawl_txt.write(article+'\\n')\n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRL7tsHKR_0h"
      },
      "source": [
        "def ruliweb_crawl(pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,\n",
        "                  user_agent,base_dir,\n",
        "                  filter=False, keywords=[]):\n",
        "  \n",
        "  main_url=\"https://bbs.ruliweb.com/community/board\"\n",
        "  base_url=main_url*1\n",
        "  base_url+=((\"/%s?page=\")%board_id)\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "      \n",
        "      try:\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "        print(\"Requesting page %s once more\"%(page))\n",
        "        time.sleep(retry_delay)\n",
        "        try:\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Canceled requesting page %s\"%(page))\n",
        "          break\n",
        "      \n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      title_soup=soup.find_all(lambda tag:tag.name=='a' and tag.get('class')==['deco'])\n",
        "      title_tags=title_soup[len(title_soup)-ruliweb_bbs_articles:]\n",
        "\n",
        "      if filter==True:\n",
        "        title_tags=keywords_filter(title_tags,keywords)\n",
        "\n",
        "      for t in title_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "\n",
        "        try:\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Requesting page %s once more\"%(dir))\n",
        "          time.sleep(retry_delay)\n",
        "          try:\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "            print(\"Canceled requesting page %s\"%(dir))\n",
        "            break\n",
        "          \n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article_tags=soup.find_all(\"div\",class_=\"view_content\")\n",
        "        article_text=''\n",
        "        for article_tag in article_tags:\n",
        "          article_text+=(article_tag.text+' ')\n",
        "        article_text=article_text.replace('\\u200b',' ')\n",
        "        article_crawl_txt.write(article_text+'\\n')\n",
        "        \n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GEJkX-tzLN"
      },
      "source": [
        "def arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,\r\n",
        "               request_delay,retry_delay,\r\n",
        "               user_agent,\r\n",
        "               base_dir,\r\n",
        "               filter=False,\r\n",
        "               keywords=[]):\r\n",
        "\r\n",
        "  main_url=\"https://arca.live\"\r\n",
        "  base_url=main_url*1\r\n",
        "  base_url+=((\"/b/%s?p=\")%board_id)\r\n",
        "\r\n",
        "  for crawl_sector_cursor in range (len(pages)):\r\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\r\n",
        "\r\n",
        "    if crawl_title==True:\r\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\r\n",
        "      \r\n",
        "    if crawl_article==True:\r\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\r\n",
        "\r\n",
        "    for page in range (crawl_min,crawl_max):\r\n",
        "      url=base_url+str(page)\r\n",
        "\r\n",
        "      try:\r\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\r\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "        print(\"Requesting page %s once more\"%(page))\r\n",
        "        time.sleep(retry_delay)\r\n",
        "        try:\r\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\r\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "          print(\"Canceled requesting page %s\"%(page))\r\n",
        "          break\r\n",
        "\r\n",
        "      soup=BeautifulSoup(html,'lxml')\r\n",
        "\r\n",
        "      title_tags=soup.find_all(attrs={'class':\"title\"})\r\n",
        "      if crawl_article==True:\r\n",
        "        link_tags=soup.find_all(lambda tag: tag.name == 'a' and tag.get('class') == ['vrow'])\r\n",
        "      \r\n",
        "      if filter==True:\r\n",
        "        title_tags=keywords_filter(title_tags,keywords)\r\n",
        "\r\n",
        "      for cursor in range (len(title_tags)):\r\n",
        "        if crawl_title==True:\r\n",
        "          title_crawl_txt.write(title_tags[cursor].text+'\\n')\r\n",
        "        if crawl_article==True:\r\n",
        "          link_crawl_txt.write(main_url+link_tags[cursor]['href']+'\\n')\r\n",
        "      time.sleep(request_delay)\r\n",
        "\r\n",
        "    if crawl_title==True:\r\n",
        "      title_crawl_txt.close()\r\n",
        "\r\n",
        "    if crawl_article==True:\r\n",
        "      link_crawl_txt.close()\r\n",
        "\r\n",
        "  if crawl_article==True:\r\n",
        "    for crawl_sector_cursor in range (len(pages)):\r\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\r\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\r\n",
        "      for dir in dirs:\r\n",
        "\r\n",
        "        try:\r\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\r\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "          print(\"Requesting page %s once more\"%(dir))\r\n",
        "          time.sleep(retry_delay)\r\n",
        "          try:\r\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\r\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "            print(\"Canceled requesting page %s\"%(dir))\r\n",
        "            break\r\n",
        "            \r\n",
        "        soup=BeautifulSoup(html,'lxml')\r\n",
        "        article_tags=soup.find_all(attrs={'class':\"fr-view article-content\"})\r\n",
        "        article_soup=BeautifulSoup(str(article_tags[0]),'lxml')\r\n",
        "        articles_in_tag=article_soup.find_all('p')\r\n",
        "        \r\n",
        "        article=\"\"\r\n",
        "        for article_in_tag in articles_in_tag:\r\n",
        "          article+=article_in_tag.text\r\n",
        "\r\n",
        "        article=article.replace('\\n',' ').replace('\\u200b',' ').replace('\\xa0','')\r\n",
        "        article_crawl_txt.write(article+'\\n')\r\n",
        "        time.sleep(request_delay)\r\n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsLvoTh6VI3f"
      },
      "source": [
        "def name_txt_files(site,board_id,pages,time_code, crawl_title, crawl_article):\n",
        "  txt_files=[[],[],[]] #Title, link, article\n",
        "\n",
        "  if crawl_title==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[0].append((\"%s %s %s title crawl %s~%s.txt\")%(time_code, site, board_id, crawl_min, crawl_max-1))\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[1].append((\"%s %s %s link crawl %s~%s.txt\")%(time_code,site, board_id, crawl_min, crawl_max-1))\n",
        "      txt_files[2].append((\"%s %s %s article crawl %s~%s.txt\")%(time_code, site, board_id, crawl_min, crawl_max-1))\n",
        "  \n",
        "  return txt_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0xx7hdPsTa8"
      },
      "source": [
        "def wordseed_reader(file_names,crawl_title,crawl_article,base_dir,pages,ignore_phrases):\n",
        "  wordseeds=[]\n",
        "  len_pages=len(pages)\n",
        "\n",
        "  for crawl_sector_cursor in range (len_pages):\n",
        "    wordseed=\"\"\n",
        "    if crawl_title==True:\n",
        "        title_crawl_txt=open(base_dir+file_names[0][crawl_sector_cursor])\n",
        "        wordseed+=title_crawl_txt.read()\n",
        "        title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "        article_crawl_txt=open(base_dir+file_names[2][crawl_sector_cursor])\n",
        "        wordseed+=article_crawl_txt.read()\n",
        "        article_crawl_txt.close()\n",
        "    wordseed=blank_emoji_deleter(wordseed)\n",
        "    if len(ignore_phrases)!=0:\n",
        "      wordseed=ignore_phrase(wordseed, ignore_phrases)\n",
        "    wordseeds.append(wordseed)\n",
        "\n",
        "  return wordseeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYNh3F5D3iB"
      },
      "source": [
        "def blank_emoji_deleter(wordseed):\n",
        "  # Delete '\\n (Enter key input)'\n",
        "  wordseed=wordseed.replace('\\s+',' ').replace('\\n',' ').replace('\\u3000',' ')\n",
        "\n",
        "  # Delete Emoji\n",
        "  wordseed=emoji.get_emoji_regexp().sub(u'',wordseed)\n",
        "\n",
        "  return wordseed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ald_ynoCQ0EX"
      },
      "source": [
        "def ignore_phrase(wordseed, phrases):\n",
        "  for phrase in phrases:\n",
        "    wordseed=wordseed.replace(phrase,'')\n",
        "  return wordseed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPfI_glewXh_"
      },
      "source": [
        "def morpheme_analysis(wordseed,user_dic,base_dir):\n",
        "  if len(user_dic)==0:\n",
        "    tagged_words=konlpy.tag.Komoran().pos(wordseed)\n",
        "  else:\n",
        "    tagged_words=konlpy.tag.Komoran(userdic=base_dir+user_dic).pos(wordseed)\n",
        "  grammar=\"\"\"\n",
        "NP: {<N.*>*<Suffix>?}   # Noun phrase\n",
        "VP: {<V.*>*}            # Verb phrase\n",
        "AP: {<A.*>*}            # Adjective phrase\n",
        "\"\"\"\n",
        "  parser=nltk.RegexpParser(grammar)\n",
        "\n",
        "  return tagged_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp2SzMygE7yq"
      },
      "source": [
        "def morpheme_tag_filtter(tagged_words,selected_tags):\n",
        "  filtered_word_seed=\"\"\n",
        "  for morpheme in tagged_words:\n",
        "    if morpheme[1] in selected_tags:\n",
        "      filtered_word_seed+=(\"%s \"%morpheme[0]) \n",
        "  return filtered_word_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQn2gb8rvoNM"
      },
      "source": [
        "def keyword_ranking(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site,show_ranking):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  wordseed_list=wordseed.split()\n",
        "  keywords=[]\n",
        "  counts=[]\n",
        "\n",
        "  for keyword in wordseed_list:\n",
        "    if keyword in keywords:\n",
        "      counts[keywords.index(keyword)]+=1\n",
        "    else:\n",
        "      keywords.append(keyword)\n",
        "      counts.append(1)\n",
        "\n",
        "  kwd_dict=dict(zip(keywords,counts))\n",
        "\n",
        "  if show_ranking==True:\n",
        "    plt_kw=[]\n",
        "    plt_ct=[]\n",
        "    for rank_cursor in range (show_rank):\n",
        "      index=counts.index(max(counts))\n",
        "      plt_kw.append(keywords.pop(index))\n",
        "      plt_ct.append(counts.pop(index))\n",
        "    plt_kw.reverse()\n",
        "    plt_ct.reverse()\n",
        "    plt.barh(plt_kw,plt_ct)\n",
        "    plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\n",
        "    plt.xlabel(\"등장 횟수\")\n",
        "    plt.savefig(base_dir+(\"%s %s %s Keyword Ranking %s~%s.png\"%(time_code,site,board_id,crawl_min,crawl_max-1)))\n",
        "    plt.show()\n",
        "\n",
        "  return kwd_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXQ7oogUM3U"
      },
      "source": [
        "def wordcloud_generator(wordseed,mask_image, site, board_id, time_code, crawl_page,fontpath,base_dir,max_keywords,font_size):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  # Word cloud generate (Komoran+user_dic)\n",
        "  icon=Image.open(base_dir+mask_image)\n",
        "  mask=Image.new(\"RGB\",icon.size, (255,255,255))\n",
        "  mask.paste(icon,icon)\n",
        "  mask=np.array(mask)\n",
        "\n",
        "  wc=WordCloud(font_path=fontpath, background_color=\"white\",max_words=max_keywords,mask=mask,max_font_size=font_size,random_state=1234567)\n",
        "\n",
        "  wc.generate_from_frequencies(wordseed)\n",
        "  wc.recolor(color_func=color_func, random_state=1234567)\n",
        "    \n",
        "  wc_dir=base_dir+(\"%s %s %s Word Cloud %s~%s.png\")%(time_code, site, board_id,crawl_min,crawl_max-1)\n",
        "  wc.to_file(wc_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0qmrOjBdTMB"
      },
      "source": [
        "def Keyword_analyzer(site,board_id,crawl_pages,user_agent,cloud_mask,crawl_title=True, crawl_article=False,\n",
        "                     show_ranking=True,show_rank=5, is_m_gallery=False,\n",
        "                     custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\n",
        "                     dc_title_tag_pos=[22,72],dc_official_announce=0,\n",
        "                     ruliweb_bbs_articles=28,\n",
        "                     random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                     selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                     max_keywords=150,font_size=400,\n",
        "                     ignore_phrases=[]):\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\n",
        "  All in one function that do webcrawl, create wordcloud, legend\n",
        "  site supports dcinside, ruliweb\n",
        "  '''\n",
        "\n",
        "  time_code=get_current_time_code()\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir)\n",
        "\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir)\n",
        "\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\n",
        "    arca_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir)\n",
        "\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AwZ_QiNK4mz"
      },
      "source": [
        "def recover_wordcloud(site,board_id,time_code,crawl_pages,cloud_mask,\n",
        "                      crawl_title=True, crawl_article=False,\n",
        "                      show_ranking=True,show_rank=5, \n",
        "                      custom_word_corpus=\"\",base_dir=\"\",\n",
        "                      random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                      selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                      max_keywords=150,font_size=400,\n",
        "                      ignore_phrases=[]):\n",
        "  \n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFHJUm6ftmKB"
      },
      "source": [
        "def Keyword_relation_analyzer(site,board_id,crawl_pages,user_agent,cloud_mask,keywords,crawl_title=True, crawl_article=False,\n",
        "                              show_ranking=True,show_rank=5, is_m_gallery=False,\n",
        "                              custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\n",
        "                              dc_title_tag_pos=[21,71],dc_official_announce=0,\n",
        "                              ruliweb_bbs_articles=28,\n",
        "                              random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                              selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                              max_keywords=150,font_size=400,\n",
        "                              ignore_phrases=[]):\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\n",
        "  All in one function that do webcrawl, create wordcloud, legend\n",
        "  site supports dcinside, ruliweb\n",
        "  '''\n",
        "\n",
        "  time_code=get_current_time_code()\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir,\n",
        "                   filter=True,keywords=keywords)\n",
        "\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\n",
        "\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\n",
        "    arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\n",
        "\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McXJGlwnuDt_"
      },
      "source": [
        "def known_keyword_ranking(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site,show_ranking,known_keyword_list):\r\n",
        "\r\n",
        "  crawl_min, crawl_max=crawl_page\r\n",
        "\r\n",
        "  wordseed.replace(\" \",\"\")\r\n",
        "\r\n",
        "  keywords=[]\r\n",
        "  counts=[]\r\n",
        "\r\n",
        "  known_keywords=np.loadtxt(base_dir+known_keyword_list, dtype=str)\r\n",
        "\r\n",
        "  for keyword in known_keywords:\r\n",
        "\r\n",
        "    counted=wordseed.count(keyword)\r\n",
        "    if counted > 0:\r\n",
        "      keywords.append(keyword)\r\n",
        "      counts.append(counted)\r\n",
        "      wordseed.replace(keyword,\"\")\r\n",
        "\r\n",
        "  kwd_dict=dict(zip(keywords,counts))\r\n",
        "\r\n",
        "  if show_ranking==True:\r\n",
        "    plt_kw=[]\r\n",
        "    plt_ct=[]\r\n",
        "    for rank_cursor in range (show_rank):\r\n",
        "      index=counts.index(max(counts))\r\n",
        "      plt_kw.append(keywords.pop(index))\r\n",
        "      plt_ct.append(counts.pop(index))\r\n",
        "    plt_kw.reverse()\r\n",
        "    plt_ct.reverse()\r\n",
        "    plt.barh(plt_kw,plt_ct)\r\n",
        "    plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\r\n",
        "    plt.xlabel(\"등장 횟수\")\r\n",
        "    plt.savefig(base_dir+(\"%s %s %s Keyword Ranking %s~%s.png\"%(time_code,site,board_id,crawl_min,crawl_max-1)))\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "  return kwd_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhcG7or5tGgZ"
      },
      "source": [
        "def recover_known_Keyword_wordcloud(site,board_id,time_code,crawl_pages,cloud_mask,\r\n",
        "                      crawl_title=True, crawl_article=False,\r\n",
        "                      show_ranking=True,show_rank=5, \r\n",
        "                      custom_word_corpus=\"\",base_dir=\"\",\r\n",
        "                      random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                      selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                      max_keywords=150,font_size=400,\r\n",
        "                      ignore_phrases=[],\r\n",
        "                      known_keyword_list=\"\"):\r\n",
        "  \r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    kw_dict=known_keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking, known_keyword_list=known_keyword_list)\r\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw60Jl0lsHrS"
      },
      "source": [
        "# ver ~1.9 legacy code\r\n",
        "\r\n",
        "def keyword_ranking_v1dx(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site):\r\n",
        "\r\n",
        "  crawl_min, crawl_max=crawl_page\r\n",
        "\r\n",
        "  wordseed_list=wordseed.split()\r\n",
        "  keywords=[]\r\n",
        "  counts=[]\r\n",
        "\r\n",
        "  for keyword in wordseed_list:\r\n",
        "    if keyword in keywords:\r\n",
        "      counts[keywords.index(keyword)]+=1\r\n",
        "    else:\r\n",
        "      keywords.append(keyword)\r\n",
        "      counts.append(1)\r\n",
        "\r\n",
        "  plt_kw=[]\r\n",
        "  plt_ct=[]\r\n",
        "\r\n",
        "  for rank_cursor in range (show_rank):\r\n",
        "    index=counts.index(max(counts))\r\n",
        "    plt_kw.append(keywords.pop(index))\r\n",
        "    plt_ct.append(counts.pop(index))\r\n",
        "  plt_kw.reverse()\r\n",
        "  plt_ct.reverse()\r\n",
        "  plt.barh(plt_kw,plt_ct)\r\n",
        "  plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\r\n",
        "  plt.xlabel(\"등장 횟수\")\r\n",
        "  plt.savefig(base_dir+(\"%s %s %s Keyword Ranking %s~%s.png\"%(time_code,site,board_id,crawl_min,crawl_max-1)))\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "def wordcloud_generator_v1dx(wordseed,mask_image, site, board_id, time_code, crawl_page,fontpath,base_dir,max_keywords,font_size):\r\n",
        "\r\n",
        "  crawl_min, crawl_max=crawl_page\r\n",
        "\r\n",
        "  # Word cloud generate (Komoran+user_dic)\r\n",
        "  icon=Image.open(base_dir+mask_image)\r\n",
        "  mask=Image.new(\"RGB\",icon.size, (255,255,255))\r\n",
        "  mask.paste(icon,icon)\r\n",
        "  mask=np.array(mask)\r\n",
        "\r\n",
        "  wc=WordCloud(font_path=fontpath, background_color=\"white\",max_words=max_keywords,mask=mask,max_font_size=font_size,random_state=1234567)\r\n",
        "\r\n",
        "  wc.generate_from_text(wordseed)\r\n",
        "  wc.recolor(color_func=color_func, random_state=1234567)\r\n",
        "    \r\n",
        "  wc_dir=base_dir+(\"%s %s %s Word Cloud %s~%s.png\")%(time_code, site, board_id,crawl_min,crawl_max-1)\r\n",
        "  wc.to_file(wc_dir)\r\n",
        "\r\n",
        "def Keyword_analyzer_v1dx(site,board_id,crawl_pages,user_agent,cloud_mask,crawl_title=True, crawl_article=False,\r\n",
        "                     show_ranking=True,show_rank=5, is_m_gallery=False,\r\n",
        "                     custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\r\n",
        "                     dc_title_tag_pos=[22,72],dc_official_announce=0,\r\n",
        "                     ruliweb_bbs_articles=28,\r\n",
        "                     random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                     selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                     max_keywords=150,font_size=400,\r\n",
        "                     ignore_phrases=[]):\r\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\r\n",
        "  All in one function that do webcrawl, create wordcloud, legend\r\n",
        "  site supports dcinside, ruliweb\r\n",
        "  '''\r\n",
        "\r\n",
        "  time_code=get_current_time_code()\r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "\r\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\r\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\r\n",
        "                   is_m_gallery,board_id,\r\n",
        "                   request_delay,retry_delay,\r\n",
        "                   dc_title_tag_pos, dc_official_announce,\r\n",
        "                   user_agent,\r\n",
        "                   base_dir)\r\n",
        "\r\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\r\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir)\r\n",
        "\r\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\r\n",
        "    arca_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir)\r\n",
        "\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)\r\n",
        "    \r\n",
        "def recover_wordcloud_v1dx(site,board_id,time_code,crawl_pages,cloud_mask,\r\n",
        "                      crawl_title=True, crawl_article=False,\r\n",
        "                      show_ranking=True,show_rank=5, \r\n",
        "                      custom_word_corpus=\"\",base_dir=\"\",\r\n",
        "                      random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                      selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                      max_keywords=150,font_size=400,\r\n",
        "                      ignore_phrases=[]):\r\n",
        "  \r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)\r\n",
        "    \r\n",
        "def Keyword_relation_analyzer_v1dx(site,board_id,crawl_pages,user_agent,cloud_mask,keywords,crawl_title=True, crawl_article=False,\r\n",
        "                              show_ranking=True,show_rank=5, is_m_gallery=False,\r\n",
        "                              custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\r\n",
        "                              dc_title_tag_pos=[21,71],dc_official_announce=0,\r\n",
        "                              ruliweb_bbs_articles=28,\r\n",
        "                              random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                              selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                              max_keywords=150,font_size=400,\r\n",
        "                              ignore_phrases=[]):\r\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\r\n",
        "  All in one function that do webcrawl, create wordcloud, legend\r\n",
        "  site supports dcinside, ruliweb\r\n",
        "  '''\r\n",
        "\r\n",
        "  time_code=get_current_time_code()\r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "\r\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\r\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\r\n",
        "                   is_m_gallery,board_id,\r\n",
        "                   request_delay,retry_delay,\r\n",
        "                   dc_title_tag_pos, dc_official_announce,\r\n",
        "                   user_agent,\r\n",
        "                   base_dir,\r\n",
        "                   filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\r\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\r\n",
        "    arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrbTb9y9M20n",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "68c144ea-7327-4b70-9977-78e976703196"
      },
      "source": [
        "# Download libraries\n",
        "base_dir=setup()\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "# For web crawling\n",
        "from bs4 import BeautifulSoup\n",
        "import requests as rq\n",
        "import re\n",
        "\n",
        "# For naming\n",
        "import datetime\n",
        "\n",
        "# For word cloud generating\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from palettable.colorbrewer.qualitative import Dark2_8\n",
        "\n",
        "# For kr analysis\n",
        "import konlpy\n",
        "import nltk\n",
        "\n",
        "# For set delay\n",
        "import time\n",
        "\n",
        "# For emoji delete\n",
        "import emoji\n",
        "\n",
        "# For draw legend\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "openjdk-8-jdk is already the newest version (8u275-b01-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n",
            "Requirement already satisfied: konlpy==0.5.1 in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy==0.5.1) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.5.7->konlpy==0.5.1) (3.7.4.3)\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV_v7lgrcN6s"
      },
      "source": [
        "'''\n",
        "# Example use for dcinside\n",
        "Keyword_analyzer(site='dc',\n",
        "                 board_id='hypergryph',\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\n",
        "                 crawl_pages=[[7,49],[49,107],[107,144]],\n",
        "                 crawl_title=True, \n",
        "                 crawl_article=False,\n",
        "                 show_ranking=True,\n",
        "                 show_rank=10,\n",
        "                 is_m_gallery=True,\n",
        "                 cloud_mask=\"PUT YOUR USER IMAGE\",\n",
        "                 custom_word_corpus=\"dc_ark_user_dic_v20210107.txt\",\n",
        "                 dc_official_announce=2,\n",
        "                 base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpNcrGONiNWj"
      },
      "source": [
        "'''\r\n",
        "# Example use for recover wordcloud and ranking with crawled txt files\r\n",
        "recover_wordcloud(site='dc',\r\n",
        "                  board_id='hypergryph',\r\n",
        "                  time_code='210107 19-52-18',\r\n",
        "                  crawl_pages=[[7,49],[49,107],[107,144]],\r\n",
        "                  cloud_mask=\"PUT YOUR USER IMAGE\",\r\n",
        "                  crawl_title=True, \r\n",
        "                  crawl_article=False,\r\n",
        "                  show_ranking=True,\r\n",
        "                  show_rank=10, \r\n",
        "                  custom_word_corpus=\"dc_ark_user_dic_v20210107.txt\",\r\n",
        "                  base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65-kY641B5th"
      },
      "source": [
        "'''\n",
        "# Example use for Ruliweb\n",
        "Keyword_analyzer(site='ruliweb',\n",
        "                 board_id='300143',\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\n",
        "                 crawl_pages=[[1,2]],\n",
        "                 crawl_title=True, \n",
        "                 crawl_article=True,\n",
        "                 show_ranking=True,\n",
        "                 show_rank=10,\n",
        "                 cloud_mask=\"PUT YOUR USER IMAGE\",\n",
        "                 custom_word_corpus=\"ruliweb_user_dic_v20201109.txt\",\n",
        "                 dc_official_announce=2,\n",
        "                 base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o3xSqL1WVs"
      },
      "source": [
        "'''# Example use for Arcalive\r\n",
        "Keyword_analyzer(site='arca',\r\n",
        "                 board_id='lastorigin',\r\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\r\n",
        "                 crawl_pages=[[2,3]],\r\n",
        "                 crawl_title=True, \r\n",
        "                 crawl_article=True,\r\n",
        "                 show_ranking=True,\r\n",
        "                 show_rank=10,\r\n",
        "                 is_m_gallery=True,\r\n",
        "                 cloud_mask=\"PUT YOUR USER IMAGE\",\r\n",
        "                 base_dir=base_dir,\r\n",
        "                 request_delay=2)                '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtcGSdsNIxpy"
      },
      "source": [
        "'''\n",
        "# Example use for recover wordcloud and ranking with crawled txt files\n",
        "recover_wordcloud(site='ruliweb',\n",
        "                  board_id='300143',\n",
        "                  time_code='201109 19-09-57',\n",
        "                  crawl_pages=[[1,2]],\n",
        "                  cloud_mask=\"PUT YOUR USER IMAGE\",\n",
        "                  crawl_title=True, \n",
        "                  crawl_article=False,\n",
        "                  show_ranking=True,\n",
        "                  show_rank=5, \n",
        "                  custom_word_corpus=\"ruliweb_user_dic_v20201109.txt\",\n",
        "                  base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRYQYaw_hPTU"
      },
      "source": [
        "'''\n",
        "# Example use for Keyword_relation_analyzer()\n",
        "Keyword_relation_analyzer_v1dx(site='dc',\n",
        "                          board_id='hypergryph',\n",
        "                          user_agent=\"PUT YOUR USER AGENT\",\n",
        "                          crawl_pages=[[1,213]],\n",
        "                          crawl_title=True, \n",
        "                          crawl_article=True,\n",
        "                          show_ranking=True,\n",
        "                          show_rank=10,\n",
        "                          is_m_gallery=True,\n",
        "                          cloud_mask=\"PUT YOUR USER IMAGE\",\n",
        "                          custom_word_corpus=\"dc_ark_user_dic_v20210122.txt\",\n",
        "                          dc_official_announce=1,\n",
        "                          base_dir=base_dir,\n",
        "                          keywords=[\"상시\",\"18점\",\"19점\",\"20점\",\"21점\",\"22점\",\"23점\",\"24점\",\"25점\",\"26점\",\"27점\"])'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t35J4EEPwRsS"
      },
      "source": [
        "'''\r\n",
        "# Example use for recover wordcloud and ranking with crawled txt files\r\n",
        "recover_known_Keyword_wordcloud(site='dc',\r\n",
        "                  board_id='hypergryph',\r\n",
        "                  time_code='210126 02-37-09',\r\n",
        "                  crawl_pages=[[1,213]],\r\n",
        "                  cloud_mask=\"PUT YOUR USER IMAGE\",\r\n",
        "                  crawl_title=True, \r\n",
        "                  crawl_article=True,\r\n",
        "                  show_ranking=True,\r\n",
        "                  show_rank=15, \r\n",
        "                  custom_word_corpus=\"dc_ark_user_dic_v20210122.txt\",\r\n",
        "                  base_dir=base_dir,\r\n",
        "                  known_keyword_list=\"ark_op_name_list 20210103.txt\")'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP5BknMGjNvK"
      },
      "source": [
        "'''\r\n",
        "# Example use for dcinside (v 1.x version)\r\n",
        "Keyword_analyzer_v1dx(site='dc',\r\n",
        "                 board_id='mibj',\r\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\r\n",
        "                 crawl_pages=[[12,123],[123,284],[284,393]],\r\n",
        "                 crawl_title=True, \r\n",
        "                 crawl_article=False,\r\n",
        "                 show_ranking=True,\r\n",
        "                 show_rank=10,\r\n",
        "                 is_m_gallery=True,\r\n",
        "                 cloud_mask=\"PUT YOUR USER IMAGE\",\r\n",
        "                 custom_word_corpus=\"dc_ark_user_dic_v20210122.txt\",\r\n",
        "                 dc_official_announce=1,\r\n",
        "                 base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}