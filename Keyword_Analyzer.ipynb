{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Analyzer v 2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6ZkQwv6gKROI83zGc6h72",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jang-KyungWuk/KR_Community_Word_Cloud_Generator/blob/master/Keyword_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F19674Lupe4w"
      },
      "source": [
        "version=2.1\n",
        "'''\n",
        "<current version 2.1>\n",
        "~1.9 version algorithm available again.\n",
        "(add _v1dx to function name)\n",
        "\n",
        "<future version 2.2 will try>\n",
        "Hyperlink based redundant-safe title crawl\n",
        "Add word2vec or LDA feature (can be postponded. needs futher study)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBUVudqsOcl8"
      },
      "source": [
        "def setup():\n",
        "  '''\n",
        "  FOR GOOGLE COLAB USERS START setup() \n",
        "  AND RESTART RUNTIME (CTRL+M) \n",
        "  THEN START setup() TO CHECK WHETHER KR FONT IS PRINTED CORRECTLY\n",
        "  '''\n",
        "  \n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  base_dir=\"/content/gdrive/My Drive/\"\n",
        "\n",
        "  # KoNLPy Install\n",
        "  !apt-get update\n",
        "  !apt-get install g++ openjdk-8-jdk \n",
        "  !pip3 install konlpy==0.5.1\n",
        "\n",
        "  # Install KR Font\n",
        "  !apt -qq -y install fonts-nanum\n",
        "  fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "  KR_Font_initializer(fontpath)\n",
        "\n",
        "  # Install emoji to delete emojis\n",
        "  !pip install emoji\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def KR_Font_initializer(fontpath):\n",
        "  # For draw legend\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib as mpl\n",
        "  import matplotlib.font_manager as fm\n",
        "\n",
        "  # Set KR Font for matplotlib\n",
        "  font=fm.FontProperties(fname=fontpath,size=20)\n",
        "  plt.rc('font',family='NanumBarunGothic')\n",
        "  mpl.font_manager._rebuild()\n",
        "\n",
        "  plt.bar(['KR font','성공'],[1,2])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZYGIatVeHZ"
      },
      "source": [
        "def get_current_time_code():\n",
        "  '''time_code()\n",
        "  Generate time code from current time\n",
        "  '''\n",
        "\n",
        "  # Get current time (KST = GMT +9)\n",
        "  KST=datetime.timezone(datetime.timedelta(hours=9))\n",
        "  now=datetime.datetime.now(KST)\n",
        "  YY=str(now.year)[2:4]\n",
        "  MM=str(now.month)\n",
        "  DD=str(now.day)\n",
        "\n",
        "  hh=str(now.hour)\n",
        "  mm=str(now.minute)\n",
        "  ss=str(now.second)\n",
        "  \n",
        "  c_time=[YY,MM,DD,hh,mm,ss]\n",
        "  for cursor in range (len(c_time)):\n",
        "    if len(c_time[cursor])==1:\n",
        "      c_time[cursor]=\"0\"+c_time[cursor]\n",
        "  time_code=''.join(c_time[0:3])+' '+'-'.join(c_time[3:6])\n",
        "\n",
        "  return time_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mEKZYziS_j"
      },
      "source": [
        "# Function for coloring\n",
        "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "  return tuple(Dark2_8.colors[random.randint(0,7)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izP03y58f4Tf"
      },
      "source": [
        "# Function to find titles at DCinside\n",
        "def dc_title_finder(tag):\n",
        "  return tag.has_attr('href') and not tag.has_attr('class') and not tag.has_attr('rel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4jxmdCHu5-1"
      },
      "source": [
        "def keywords_filter(tags,keywords):\n",
        "  filtered_tags=[]\n",
        "  for tag in tags:\n",
        "    if any(keyword in tag.text for keyword in keywords)==True:\n",
        "      filtered_tags.append(tag)\n",
        "  return filtered_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1YtK8QKT3q"
      },
      "source": [
        "def dcinside_crawl(pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir,\n",
        "                   filter=False,\n",
        "                   keywords=[]):\n",
        "\n",
        "  main_url=\"https://gall.dcinside.com\"\n",
        "  base_url=main_url*1\n",
        "  if is_m_gallery==True:\n",
        "    base_url+=\"/mgallery\"\n",
        "  base_url+=((\"/board/lists/?id=%s&page=\")%board_id)\n",
        "\n",
        "  title_tag_pos_min, title_tag_pos_max=dc_title_tag_pos[0]+dc_official_announce, dc_title_tag_pos[1]+dc_official_announce\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "\n",
        "      try:\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "        print(\"Requesting page %s once more\"%(page))\n",
        "        time.sleep(retry_delay)\n",
        "        try:\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Canceled requesting page %s\"%(page))\n",
        "      \n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      tags=soup.find_all(dc_title_finder)\n",
        "      bbs_tags=tags[title_tag_pos_min:title_tag_pos_max]\n",
        "      \n",
        "      if filter==True:\n",
        "        bbs_tags=keywords_filter(bbs_tags,keywords)\n",
        "\n",
        "      for t in bbs_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(main_url+t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "        \n",
        "        try:\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Requesting page %s once more\"%(dir))\n",
        "          time.sleep(retry_delay)\n",
        "          try:\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "            print(\"Canceled requesting page %s\"%(dir))\n",
        "        \n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article=soup.body.find('div',{'style':'overflow:hidden;width:900px'}).text\n",
        "        article=article.replace('\\n',' ').replace('\\u200b',' ').replace('\\xa0','').replace('- dc official App','')\n",
        "        article_crawl_txt.write(article+'\\n')\n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRL7tsHKR_0h"
      },
      "source": [
        "def ruliweb_crawl(pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,\n",
        "                  user_agent,base_dir,\n",
        "                  filter=False, keywords=[]):\n",
        "  \n",
        "  main_url=\"https://bbs.ruliweb.com/community/board\"\n",
        "  base_url=main_url*1\n",
        "  base_url+=((\"/%s?page=\")%board_id)\n",
        "\n",
        "  for crawl_sector_cursor in range (len(pages)):\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\n",
        "      \n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\n",
        "\n",
        "\n",
        "    for page in range (crawl_min,crawl_max):\n",
        "      url=base_url+str(page)\n",
        "      \n",
        "      try:\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "        print(\"Requesting page %s once more\"%(page))\n",
        "        time.sleep(retry_delay)\n",
        "        try:\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Canceled requesting page %s\"%(page))\n",
        "          break\n",
        "      \n",
        "      soup=BeautifulSoup(html,'lxml')\n",
        "      title_soup=soup.find_all(lambda tag:tag.name=='a' and tag.get('class')==['deco'])\n",
        "      title_tags=title_soup[len(title_soup)-ruliweb_bbs_articles:]\n",
        "\n",
        "      if filter==True:\n",
        "        title_tags=keywords_filter(title_tags,keywords)\n",
        "\n",
        "      for t in title_tags:\n",
        "        if crawl_title==True:\n",
        "          title_crawl_txt.write(t.text+'\\n')\n",
        "        if crawl_article==True:\n",
        "          link_crawl_txt.write(t['href']+'\\n')\n",
        "      time.sleep(request_delay)\n",
        "\n",
        "    if crawl_title==True:\n",
        "      title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "      link_crawl_txt.close()\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_sector_cursor in range (len(pages)):\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\n",
        "      for dir in dirs:\n",
        "\n",
        "        try:\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "          print(\"Requesting page %s once more\"%(dir))\n",
        "          time.sleep(retry_delay)\n",
        "          try:\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\n",
        "            print(\"Canceled requesting page %s\"%(dir))\n",
        "            break\n",
        "          \n",
        "        soup=BeautifulSoup(html,'lxml')\n",
        "        article_tags=soup.find_all(\"div\",class_=\"view_content\")\n",
        "        article_text=''\n",
        "        for article_tag in article_tags:\n",
        "          article_text+=(article_tag.text+' ')\n",
        "        article_text=article_text.replace('\\u200b',' ')\n",
        "        article_crawl_txt.write(article_text+'\\n')\n",
        "        \n",
        "        time.sleep(request_delay)\n",
        "      \n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GEJkX-tzLN"
      },
      "source": [
        "def arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,\r\n",
        "               request_delay,retry_delay,\r\n",
        "               user_agent,\r\n",
        "               base_dir,\r\n",
        "               filter=False,\r\n",
        "               keywords=[]):\r\n",
        "\r\n",
        "  main_url=\"https://arca.live\"\r\n",
        "  base_url=main_url*1\r\n",
        "  base_url+=((\"/b/%s?p=\")%board_id)\r\n",
        "\r\n",
        "  for crawl_sector_cursor in range (len(pages)):\r\n",
        "    crawl_min,crawl_max=pages[crawl_sector_cursor]\r\n",
        "\r\n",
        "    if crawl_title==True:\r\n",
        "      title_crawl_txt=open(base_dir + txt_files[0][crawl_sector_cursor],'w')\r\n",
        "      \r\n",
        "    if crawl_article==True:\r\n",
        "      link_crawl_txt=open(base_dir + txt_files[1][crawl_sector_cursor],'w')\r\n",
        "\r\n",
        "    for page in range (crawl_min,crawl_max):\r\n",
        "      url=base_url+str(page)\r\n",
        "\r\n",
        "      try:\r\n",
        "        html=rq.get(url, headers={'User-Agent': user_agent}).text\r\n",
        "      except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "        print(\"Requesting page %s once more\"%(page))\r\n",
        "        time.sleep(retry_delay)\r\n",
        "        try:\r\n",
        "          html=rq.get(url, headers={'User-Agent': user_agent}).text\r\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "          print(\"Canceled requesting page %s\"%(page))\r\n",
        "          break\r\n",
        "\r\n",
        "      soup=BeautifulSoup(html,'lxml')\r\n",
        "\r\n",
        "      title_tags=soup.find_all(attrs={'class':\"title\"})\r\n",
        "      if crawl_article==True:\r\n",
        "        link_tags=soup.find_all(lambda tag: tag.name == 'a' and tag.get('class') == ['vrow'])\r\n",
        "      \r\n",
        "      if filter==True:\r\n",
        "        title_tags=keywords_filter(title_tags,keywords)\r\n",
        "\r\n",
        "      for cursor in range (len(title_tags)):\r\n",
        "        if crawl_title==True:\r\n",
        "          title_crawl_txt.write(title_tags[cursor].text+'\\n')\r\n",
        "        if crawl_article==True:\r\n",
        "          link_crawl_txt.write(main_url+link_tags[cursor]['href']+'\\n')\r\n",
        "      time.sleep(request_delay)\r\n",
        "\r\n",
        "    if crawl_title==True:\r\n",
        "      title_crawl_txt.close()\r\n",
        "\r\n",
        "    if crawl_article==True:\r\n",
        "      link_crawl_txt.close()\r\n",
        "\r\n",
        "  if crawl_article==True:\r\n",
        "    for crawl_sector_cursor in range (len(pages)):\r\n",
        "      dirs=np.loadtxt(base_dir+txt_files[1][crawl_sector_cursor],delimiter=\"\\n\", dtype=\"str\")\r\n",
        "      article_crawl_txt=open(base_dir+txt_files[2][crawl_sector_cursor],'w')\r\n",
        "      for dir in dirs:\r\n",
        "\r\n",
        "        try:\r\n",
        "          html=rq.get(dir, headers={'User-Agent':user_agent}).text\r\n",
        "        except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "          print(\"Requesting page %s once more\"%(dir))\r\n",
        "          time.sleep(retry_delay)\r\n",
        "          try:\r\n",
        "            html=rq.get(dir, headers={'User-Agent':user_agent}).text\r\n",
        "          except (ProtocolError, ConnectionError, RemoteDisconnected) as error:\r\n",
        "            print(\"Canceled requesting page %s\"%(dir))\r\n",
        "            break\r\n",
        "            \r\n",
        "        soup=BeautifulSoup(html,'lxml')\r\n",
        "        article_tags=soup.find_all(attrs={'class':\"fr-view article-content\"})\r\n",
        "        article_soup=BeautifulSoup(str(article_tags[0]),'lxml')\r\n",
        "        articles_in_tag=article_soup.find_all('p')\r\n",
        "        \r\n",
        "        article=\"\"\r\n",
        "        for article_in_tag in articles_in_tag:\r\n",
        "          article+=article_in_tag.text\r\n",
        "\r\n",
        "        article=article.replace('\\n',' ').replace('\\u200b',' ').replace('\\xa0','')\r\n",
        "        article_crawl_txt.write(article+'\\n')\r\n",
        "        time.sleep(request_delay)\r\n",
        "      article_crawl_txt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsLvoTh6VI3f"
      },
      "source": [
        "def name_txt_files(site,board_id,pages,time_code, crawl_title, crawl_article):\n",
        "  txt_files=[[],[],[]] #Title, link, article\n",
        "\n",
        "  if crawl_title==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[0].append((\"%s %s %s title crawl %s~%s.txt\")%(time_code, site, board_id, crawl_min, crawl_max-1))\n",
        "\n",
        "  if crawl_article==True:\n",
        "    for crawl_page in pages:\n",
        "      crawl_min,crawl_max=crawl_page\n",
        "      txt_files[1].append((\"%s %s %s link crawl %s~%s.txt\")%(time_code,site, board_id, crawl_min, crawl_max-1))\n",
        "      txt_files[2].append((\"%s %s %s article crawl %s~%s.txt\")%(time_code, site, board_id, crawl_min, crawl_max-1))\n",
        "  \n",
        "  return txt_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0xx7hdPsTa8"
      },
      "source": [
        "def wordseed_reader(file_names,crawl_title,crawl_article,base_dir,pages,ignore_phrases):\n",
        "  wordseeds=[]\n",
        "  len_pages=len(pages)\n",
        "\n",
        "  for crawl_sector_cursor in range (len_pages):\n",
        "    wordseed=\"\"\n",
        "    if crawl_title==True:\n",
        "        title_crawl_txt=open(base_dir+file_names[0][crawl_sector_cursor])\n",
        "        wordseed+=title_crawl_txt.read()\n",
        "        title_crawl_txt.close()\n",
        "\n",
        "    if crawl_article==True:\n",
        "        article_crawl_txt=open(base_dir+file_names[2][crawl_sector_cursor])\n",
        "        wordseed+=article_crawl_txt.read()\n",
        "        article_crawl_txt.close()\n",
        "    wordseed=blank_emoji_deleter(wordseed)\n",
        "    if len(ignore_phrases)!=0:\n",
        "      wordseed=ignore_phrase(wordseed, ignore_phrases)\n",
        "    wordseeds.append(wordseed)\n",
        "\n",
        "  return wordseeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYNh3F5D3iB"
      },
      "source": [
        "def blank_emoji_deleter(wordseed):\n",
        "  # Delete '\\n (Enter key input)'\n",
        "  wordseed=wordseed.replace('\\s+',' ').replace('\\n',' ').replace('\\u3000',' ')\n",
        "\n",
        "  # Delete Emoji\n",
        "  wordseed=emoji.get_emoji_regexp().sub(u'',wordseed)\n",
        "\n",
        "  return wordseed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ald_ynoCQ0EX"
      },
      "source": [
        "def ignore_phrase(wordseed, phrases):\n",
        "  for phrase in phrases:\n",
        "    wordseed=wordseed.replace(phrase,'')\n",
        "  return wordseed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPfI_glewXh_"
      },
      "source": [
        "def morpheme_analysis(wordseed,user_dic,base_dir):\n",
        "  if len(user_dic)==0:\n",
        "    tagged_words=konlpy.tag.Komoran().pos(wordseed)\n",
        "  else:\n",
        "    tagged_words=konlpy.tag.Komoran(userdic=base_dir+user_dic).pos(wordseed)\n",
        "  grammar=\"\"\"\n",
        "NP: {<N.*>*<Suffix>?}   # Noun phrase\n",
        "VP: {<V.*>*}            # Verb phrase\n",
        "AP: {<A.*>*}            # Adjective phrase\n",
        "\"\"\"\n",
        "  parser=nltk.RegexpParser(grammar)\n",
        "\n",
        "  return tagged_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp2SzMygE7yq"
      },
      "source": [
        "def morpheme_tag_filtter(tagged_words,selected_tags):\n",
        "  filtered_word_seed=\"\"\n",
        "  for morpheme in tagged_words:\n",
        "    if morpheme[1] in selected_tags:\n",
        "      filtered_word_seed+=(\"%s \"%morpheme[0]) \n",
        "  return filtered_word_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQn2gb8rvoNM"
      },
      "source": [
        "def keyword_ranking(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site,show_ranking):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  wordseed_list=wordseed.split()\n",
        "  keywords=[]\n",
        "  counts=[]\n",
        "\n",
        "  for keyword in wordseed_list:\n",
        "    if keyword in keywords:\n",
        "      counts[keywords.index(keyword)]+=1\n",
        "    else:\n",
        "      keywords.append(keyword)\n",
        "      counts.append(1)\n",
        "\n",
        "  kwd_dict=dict(zip(keywords,counts))\n",
        "\n",
        "  if show_ranking==True:\n",
        "    plt_kw=[]\n",
        "    plt_ct=[]\n",
        "    for rank_cursor in range (show_rank):\n",
        "      index=counts.index(max(counts))\n",
        "      plt_kw.append(keywords.pop(index))\n",
        "      plt_ct.append(counts.pop(index))\n",
        "    plt_kw.reverse()\n",
        "    plt_ct.reverse()\n",
        "    plt.barh(plt_kw,plt_ct)\n",
        "    plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\n",
        "    plt.xlabel(\"등장 횟수\")\n",
        "    plt.savefig(base_dir+(\"%s %s %s Keyword Ranking %s~%s.png\"%(time_code,site,board_id,crawl_min,crawl_max-1)))\n",
        "    plt.show()\n",
        "\n",
        "  return kwd_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXQ7oogUM3U"
      },
      "source": [
        "def wordcloud_generator(wordseed,mask_image, site, board_id, time_code, crawl_page,fontpath,base_dir,max_keywords,font_size):\n",
        "\n",
        "  crawl_min, crawl_max=crawl_page\n",
        "\n",
        "  # Word cloud generate (Komoran+user_dic)\n",
        "  icon=Image.open(base_dir+mask_image)\n",
        "  mask=Image.new(\"RGB\",icon.size, (255,255,255))\n",
        "  mask.paste(icon,icon)\n",
        "  mask=np.array(mask)\n",
        "\n",
        "  wc=WordCloud(font_path=fontpath, background_color=\"white\",max_words=max_keywords,mask=mask,max_font_size=font_size,random_state=1234567)\n",
        "\n",
        "  wc.generate_from_frequencies(wordseed)\n",
        "  wc.recolor(color_func=color_func, random_state=1234567)\n",
        "    \n",
        "  wc_dir=base_dir+(\"%s %s %s Word Cloud %s~%s.png\")%(time_code, site, board_id,crawl_min,crawl_max-1)\n",
        "  wc.to_file(wc_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0qmrOjBdTMB"
      },
      "source": [
        "def Keyword_analyzer(site,board_id,crawl_pages,user_agent,cloud_mask,crawl_title=True, crawl_article=False,\n",
        "                     show_ranking=True,show_rank=5, is_m_gallery=False,\n",
        "                     custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\n",
        "                     dc_title_tag_pos=[22,72],dc_official_announce=0,\n",
        "                     ruliweb_bbs_articles=28,\n",
        "                     random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                     selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                     max_keywords=150,font_size=400,\n",
        "                     ignore_phrases=[]):\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\n",
        "  All in one function that do webcrawl, create wordcloud, legend\n",
        "  site supports dcinside, ruliweb\n",
        "  '''\n",
        "\n",
        "  time_code=get_current_time_code()\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir)\n",
        "\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir)\n",
        "\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\n",
        "    arca_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir)\n",
        "\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AwZ_QiNK4mz"
      },
      "source": [
        "def recover_wordcloud(site,board_id,time_code,crawl_pages,cloud_mask,\n",
        "                      crawl_title=True, crawl_article=False,\n",
        "                      show_ranking=True,show_rank=5, \n",
        "                      custom_word_corpus=\"\",base_dir=\"\",\n",
        "                      random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                      selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                      max_keywords=150,font_size=400,\n",
        "                      ignore_phrases=[]):\n",
        "  \n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFHJUm6ftmKB"
      },
      "source": [
        "def Keyword_relation_analyzer(site,board_id,crawl_pages,user_agent,cloud_mask,keywords,crawl_title=True, crawl_article=False,\n",
        "                              show_ranking=True,show_rank=5, is_m_gallery=False,\n",
        "                              custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\n",
        "                              dc_title_tag_pos=[21,71],dc_official_announce=0,\n",
        "                              ruliweb_bbs_articles=28,\n",
        "                              random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                              selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\n",
        "                              max_keywords=150,font_size=400,\n",
        "                              ignore_phrases=[]):\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\n",
        "  All in one function that do webcrawl, create wordcloud, legend\n",
        "  site supports dcinside, ruliweb\n",
        "  '''\n",
        "\n",
        "  time_code=get_current_time_code()\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\n",
        "\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\n",
        "                   is_m_gallery,board_id,\n",
        "                   request_delay,retry_delay,\n",
        "                   dc_title_tag_pos, dc_official_announce,\n",
        "                   user_agent,\n",
        "                   base_dir,\n",
        "                   filter=True,keywords=keywords)\n",
        "\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\n",
        "\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\n",
        "    arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\n",
        "\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\n",
        "\n",
        "\n",
        "  for file_cursor in range(len(crawl_pages)):\n",
        "\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\n",
        "\n",
        "    kw_dict=keyword_ranking(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site, show_ranking=show_ranking)\n",
        "    wordcloud_generator(wordseed=kw_dict,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw60Jl0lsHrS"
      },
      "source": [
        "# ver ~1.9 legacy code\r\n",
        "\r\n",
        "def keyword_ranking_v1dx(wordseed,show_rank,board_id,time_code,crawl_page,base_dir,site):\r\n",
        "\r\n",
        "  crawl_min, crawl_max=crawl_page\r\n",
        "\r\n",
        "  wordseed_list=wordseed.split()\r\n",
        "  keywords=[]\r\n",
        "  counts=[]\r\n",
        "\r\n",
        "  for keyword in wordseed_list:\r\n",
        "    if keyword in keywords:\r\n",
        "      counts[keywords.index(keyword)]+=1\r\n",
        "    else:\r\n",
        "      keywords.append(keyword)\r\n",
        "      counts.append(1)\r\n",
        "\r\n",
        "  plt_kw=[]\r\n",
        "  plt_ct=[]\r\n",
        "\r\n",
        "  for rank_cursor in range (show_rank):\r\n",
        "    index=counts.index(max(counts))\r\n",
        "    plt_kw.append(keywords.pop(index))\r\n",
        "    plt_ct.append(counts.pop(index))\r\n",
        "  plt_kw.reverse()\r\n",
        "  plt_ct.reverse()\r\n",
        "  plt.barh(plt_kw,plt_ct)\r\n",
        "  plt.title(\"상위 %s개 키워드 등장 횟수\"%(show_rank))\r\n",
        "  plt.xlabel(\"등장 횟수\")\r\n",
        "  plt.savefig(base_dir+(\"%s %s %s Keyword Ranking %s~%s.png\"%(time_code,site,board_id,crawl_min,crawl_max-1)))\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "def wordcloud_generator_v1dx(wordseed,mask_image, site, board_id, time_code, crawl_page,fontpath,base_dir,max_keywords,font_size):\r\n",
        "\r\n",
        "  crawl_min, crawl_max=crawl_page\r\n",
        "\r\n",
        "  # Word cloud generate (Komoran+user_dic)\r\n",
        "  icon=Image.open(base_dir+mask_image)\r\n",
        "  mask=Image.new(\"RGB\",icon.size, (255,255,255))\r\n",
        "  mask.paste(icon,icon)\r\n",
        "  mask=np.array(mask)\r\n",
        "\r\n",
        "  wc=WordCloud(font_path=fontpath, background_color=\"white\",max_words=max_keywords,mask=mask,max_font_size=font_size,random_state=1234567)\r\n",
        "\r\n",
        "  wc.generate_from_text(wordseed)\r\n",
        "  wc.recolor(color_func=color_func, random_state=1234567)\r\n",
        "    \r\n",
        "  wc_dir=base_dir+(\"%s %s %s Word Cloud %s~%s.png\")%(time_code, site, board_id,crawl_min,crawl_max-1)\r\n",
        "  wc.to_file(wc_dir)\r\n",
        "\r\n",
        "def Keyword_analyzer_v1dx(site,board_id,crawl_pages,user_agent,cloud_mask,crawl_title=True, crawl_article=False,\r\n",
        "                     show_ranking=True,show_rank=5, is_m_gallery=False,\r\n",
        "                     custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\r\n",
        "                     dc_title_tag_pos=[22,72],dc_official_announce=0,\r\n",
        "                     ruliweb_bbs_articles=28,\r\n",
        "                     random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                     selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                     max_keywords=150,font_size=400,\r\n",
        "                     ignore_phrases=[]):\r\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\r\n",
        "  All in one function that do webcrawl, create wordcloud, legend\r\n",
        "  site supports dcinside, ruliweb\r\n",
        "  '''\r\n",
        "\r\n",
        "  time_code=get_current_time_code()\r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "\r\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\r\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\r\n",
        "                   is_m_gallery,board_id,\r\n",
        "                   request_delay,retry_delay,\r\n",
        "                   dc_title_tag_pos, dc_official_announce,\r\n",
        "                   user_agent,\r\n",
        "                   base_dir)\r\n",
        "\r\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\r\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir)\r\n",
        "\r\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\r\n",
        "    arca_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir)\r\n",
        "\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)\r\n",
        "    \r\n",
        "def recover_wordcloud_v1dx(site,board_id,time_code,crawl_pages,cloud_mask,\r\n",
        "                      crawl_title=True, crawl_article=False,\r\n",
        "                      show_ranking=True,show_rank=5, \r\n",
        "                      custom_word_corpus=\"\",base_dir=\"\",\r\n",
        "                      random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                      selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                      max_keywords=150,font_size=400,\r\n",
        "                      ignore_phrases=[]):\r\n",
        "  \r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)\r\n",
        "    \r\n",
        "def Keyword_relation_analyzer_v1dx(site,board_id,crawl_pages,user_agent,cloud_mask,keywords,crawl_title=True, crawl_article=False,\r\n",
        "                              show_ranking=True,show_rank=5, is_m_gallery=False,\r\n",
        "                              custom_word_corpus=\"\",base_dir=\"\", request_delay=1,retry_delay=5,\r\n",
        "                              dc_title_tag_pos=[21,71],dc_official_announce=0,\r\n",
        "                              ruliweb_bbs_articles=28,\r\n",
        "                              random_seed='1234567',fontpath='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\r\n",
        "                              selected_tags=[\"NNG\",\"NNP\",\"NR\",\"SL\",\"SH\",\"SW\",\"SN\"],\r\n",
        "                              max_keywords=150,font_size=400,\r\n",
        "                              ignore_phrases=[]):\r\n",
        "  '''Keyword_analyzer(site,pages,board_id,crawl_title=True, crawl_article_False, is_m_gallery=False)\r\n",
        "  All in one function that do webcrawl, create wordcloud, legend\r\n",
        "  site supports dcinside, ruliweb\r\n",
        "  '''\r\n",
        "\r\n",
        "  time_code=get_current_time_code()\r\n",
        "  txt_files=name_txt_files(site,board_id,crawl_pages,time_code, crawl_title, crawl_article)\r\n",
        "\r\n",
        "  if site in ([\"DCinside\",\"DC\",\"dc\",\"dcinside\",\"dcinside.com\"]):\r\n",
        "    dcinside_crawl(crawl_pages,txt_files,crawl_title,crawl_article,\r\n",
        "                   is_m_gallery,board_id,\r\n",
        "                   request_delay,retry_delay,\r\n",
        "                   dc_title_tag_pos, dc_official_announce,\r\n",
        "                   user_agent,\r\n",
        "                   base_dir,\r\n",
        "                   filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  elif site in ([\"Ruliweb\",\"ruliweb\",\"ruliweb.com\"]):\r\n",
        "    ruliweb_crawl(crawl_pages,txt_files,crawl_title,crawl_article,board_id,ruliweb_bbs_articles,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  elif site in ([\"arca\",\"arcalive\",\"arca live\",\"arca.live\"]):\r\n",
        "    arca_crawl(pages,txt_files,crawl_title,crawl_article,board_id,request_delay,retry_delay,user_agent,base_dir,filter=True,keywords=keywords)\r\n",
        "\r\n",
        "  wordseeds=wordseed_reader(file_names=txt_files,crawl_title=crawl_title,crawl_article=crawl_article,base_dir=base_dir,pages=crawl_pages,ignore_phrases=ignore_phrases)\r\n",
        "\r\n",
        "\r\n",
        "  for file_cursor in range(len(crawl_pages)):\r\n",
        "\r\n",
        "    tagged_wordseed=morpheme_analysis(wordseeds[file_cursor],custom_word_corpus,base_dir)\r\n",
        "    filttered_wordseed=morpheme_tag_filtter(tagged_wordseed,selected_tags)\r\n",
        "\r\n",
        "    if show_ranking==True:\r\n",
        "      keyword_ranking_v1dx(wordseed=filttered_wordseed,show_rank=show_rank,board_id=board_id,time_code=time_code,crawl_page=crawl_pages[file_cursor],base_dir=base_dir, site=site)\r\n",
        "    \r\n",
        "    wordcloud_generator_v1dx(wordseed=filttered_wordseed,mask_image=cloud_mask, site=site, board_id=board_id, time_code=time_code,crawl_page=crawl_pages[file_cursor],fontpath=fontpath,base_dir=base_dir,\r\n",
        "                        max_keywords=max_keywords,font_size=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrbTb9y9M20n",
        "cellView": "both"
      },
      "source": [
        "# Download libraries\n",
        "base_dir=setup()\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "# For web crawling\n",
        "from bs4 import BeautifulSoup\n",
        "import requests as rq\n",
        "import re\n",
        "\n",
        "# For naming\n",
        "import datetime\n",
        "\n",
        "# For word cloud generating\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from palettable.colorbrewer.qualitative import Dark2_8\n",
        "\n",
        "# For kr analysis\n",
        "import konlpy\n",
        "import nltk\n",
        "\n",
        "# For set delay\n",
        "import time\n",
        "\n",
        "# For emoji delete\n",
        "import emoji\n",
        "\n",
        "# For draw legend\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV_v7lgrcN6s"
      },
      "source": [
        "'''\n",
        "# Example use for dcinside\n",
        "Keyword_analyzer(site='dc',\n",
        "                 board_id='hypergryph',\n",
        "                 user_agent=\"el anemos keyword analyzer for word cloud | contact elixir95@naver.com\",\n",
        "                 crawl_pages=[[7,49],[49,107],[107,144]],\n",
        "                 crawl_title=True, \n",
        "                 crawl_article=False,\n",
        "                 show_ranking=True,\n",
        "                 show_rank=10,\n",
        "                 is_m_gallery=True,\n",
        "                 cloud_mask='archetto_b.png',\n",
        "                 custom_word_corpus=\"dc_ark_user_dic_v20210107.txt\",\n",
        "                 dc_official_announce=2,\n",
        "                 base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpNcrGONiNWj"
      },
      "source": [
        "'''\r\n",
        "# Example use for recover wordcloud and ranking with crawled txt files\r\n",
        "recover_wordcloud(site='dc',\r\n",
        "                  board_id='hypergryph',\r\n",
        "                  time_code='210107 19-52-18',\r\n",
        "                  crawl_pages=[[7,49],[49,107],[107,144]],\r\n",
        "                  cloud_mask='archetto_b.png',\r\n",
        "                  crawl_title=True, \r\n",
        "                  crawl_article=False,\r\n",
        "                  show_ranking=True,\r\n",
        "                  show_rank=10, \r\n",
        "                  custom_word_corpus=\"dc_ark_user_dic_v20210107.txt\",\r\n",
        "                  base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65-kY641B5th"
      },
      "source": [
        "'''\n",
        "# Example use for Ruliweb\n",
        "Keyword_analyzer(site='ruliweb',\n",
        "                 board_id='300143',\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\n",
        "                 crawl_pages=[[1,2]],\n",
        "                 crawl_title=True, \n",
        "                 crawl_article=True,\n",
        "                 show_ranking=True,\n",
        "                 show_rank=10,\n",
        "                 cloud_mask='suzuran_b.png',\n",
        "                 custom_word_corpus=\"ruliweb_user_dic_v20201109.txt\",\n",
        "                 dc_official_announce=2,\n",
        "                 base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o3xSqL1WVs"
      },
      "source": [
        "'''# Example use for Arcalive\r\n",
        "Keyword_analyzer(site='arca',\r\n",
        "                 board_id='lastorigin',\r\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\r\n",
        "                 crawl_pages=[[2,3]],\r\n",
        "                 crawl_title=True, \r\n",
        "                 crawl_article=True,\r\n",
        "                 show_ranking=True,\r\n",
        "                 show_rank=10,\r\n",
        "                 is_m_gallery=True,\r\n",
        "                 cloud_mask='Mountain_b.png',\r\n",
        "                 base_dir=base_dir,\r\n",
        "                 request_delay=2)                '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtcGSdsNIxpy"
      },
      "source": [
        "'''\n",
        "# Example use for recover wordcloud and ranking with crawled txt files\n",
        "recover_wordcloud(site='ruliweb',\n",
        "                  board_id='300143',\n",
        "                  time_code='201109 19-09-57',\n",
        "                  crawl_pages=[[1,2]],\n",
        "                  cloud_mask='suzuran_b.png',\n",
        "                  crawl_title=True, \n",
        "                  crawl_article=False,\n",
        "                  show_ranking=True,\n",
        "                  show_rank=5, \n",
        "                  custom_word_corpus=\"ruliweb_user_dic_v20201109.txt\",\n",
        "                  base_dir=base_dir)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRYQYaw_hPTU"
      },
      "source": [
        "'''\n",
        "# Example use for Keyword_relation_analyzer()\n",
        "Keyword_relation_analyzer(site='dc',\n",
        "                          board_id='hypergryph',\n",
        "                          user_agent=\"PUT YOUR USER AGENT\",\n",
        "                          crawl_pages=[[10,83]],\n",
        "                          crawl_title=True, \n",
        "                          crawl_article=True,\n",
        "                          show_ranking=True,\n",
        "                          show_rank=10,\n",
        "                          is_m_gallery=True,\n",
        "                          cloud_mask='bagpipe whistlewind_b.png',\n",
        "                          custom_word_corpus=\"dc_ark_user_dic_v20201117.txt\",\n",
        "                          dc_official_announce=1,\n",
        "                          base_dir=base_dir,\n",
        "                          keywords=[\"상시\",\"18점\",\"19점\",\"20점\",\"21점\",\"22점\",\"23점\",\"24점\"])'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP5BknMGjNvK"
      },
      "source": [
        "# Example use for dcinside (v 1.x version)\r\n",
        "Keyword_analyzer_v1dx(site='dc',\r\n",
        "                 board_id='mibj',\r\n",
        "                 user_agent=\"PUT YOUR USER AGENT\",\r\n",
        "                 crawl_pages=[[12,123],[123,284],[284,393]],\r\n",
        "                 crawl_title=True, \r\n",
        "                 crawl_article=False,\r\n",
        "                 show_ranking=True,\r\n",
        "                 show_rank=10,\r\n",
        "                 is_m_gallery=True,\r\n",
        "                 cloud_mask='Thorns_b.png',\r\n",
        "                 custom_word_corpus=\"dc_ark_user_dic_v20210122.txt\",\r\n",
        "                 dc_official_announce=1,\r\n",
        "                 base_dir=base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}